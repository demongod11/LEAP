{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from cs_dataset import *\n",
    "from loss_function import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import csv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pickle\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotChart(x,y,xlabel,ylabel,leg_label,title):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    plt.plot(x,y, label=leg_label)\n",
    "    leg = plt.legend(loc='best', ncol=2, shadow=True, fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.xlabel(xlabel, weight='bold')\n",
    "    plt.ylabel(ylabel, weight='bold')\n",
    "    plt.title(title,weight='bold')\n",
    "    plt.savefig(title+'.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "alpha = 0.8\n",
    "hidden1_dim = 64\n",
    "hidden2_dim = 32\n",
    "\n",
    "folderPath = os.environ.get(\"HOME\")+\"/Priority-Cuts/Priority-Cuts-Filter/data/square/\"\n",
    "cutstats_dataset = CutStatsDataset(folderPath=folderPath)\n",
    "training_validation_size = [int(0.9*len(cutstats_dataset)),len(cutstats_dataset) - int(0.9*len(cutstats_dataset))]\n",
    "train_DS,valid_DS = random_split(cutstats_dataset,training_validation_size)\n",
    "\n",
    "train_dl = DataLoader(train_DS,shuffle=True,batch_size=batch_size,pin_memory=True,num_workers=4)\n",
    "valid_dl = DataLoader(valid_DS,shuffle=True,batch_size=batch_size,pin_memory=True,num_workers=4)\n",
    "\n",
    "delay_model = DelayPredictor(hidden1_dim=hidden1_dim, hidden2_dim=hidden2_dim).to(device)\n",
    "optimizer = torch.optim.Adam(delay_model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',verbose=True)\n",
    "    \n",
    "best_val_epoch = 1 \n",
    "valid_curve = []\n",
    "train_curve = []\n",
    "optim_valid_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.0289, 0.9711]], device='cuda:0')\n",
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "num_positive = cutstats_dataset.num_positive\n",
    "num_negative = len(cutstats_dataset) - num_positive\n",
    "weight_positive = num_negative / len(cutstats_dataset)\n",
    "weight_negative = num_positive / len(cutstats_dataset)\n",
    "class_weights = torch.tensor([[weight_negative, weight_positive]] * batch_size).to(device)\n",
    "loss_function = LossFunction(class_weights=class_weights,alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/50]\n",
      "\n",
      "Training..\n",
      "Epoch [1/50], Step [100/372], Loss: 0.2132\n",
      "Epoch [1/50], Step [200/372], Loss: 0.2151\n",
      "Epoch [1/50], Step [300/372], Loss: 0.2165\n",
      "\n",
      "Validation..\n",
      "Training loss for epoch 1 is 0.2165\n",
      "Validation loss for epoch 1 is 0.2180\n",
      "\n",
      "Epoch [2/50]\n",
      "\n",
      "Training..\n",
      "Epoch [2/50], Step [100/372], Loss: 0.2177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m class_pred \u001b[38;5;241m=\u001b[39m class_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m reg_pred \u001b[38;5;241m=\u001b[39m reg_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_class_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_reg_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/priority_cuts/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Priority-Cuts/Priority-Cuts-Filter/cut-sol/loss_function.py:20\u001b[0m, in \u001b[0;36mLossFunction.forward\u001b[0;34m(self, class_output, reg_output, class_target, reg_target)\u001b[0m\n\u001b[1;32m     17\u001b[0m new_reg_target \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_target)):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mclass_target\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m         new_reg_output\u001b[38;5;241m.\u001b[39mappend(reg_output[i]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     22\u001b[0m         new_reg_target\u001b[38;5;241m.\u001b[39mappend(reg_target[i]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nEpoch [{}/{}]\".format(ep+1, num_epochs))   \n",
    "    print(\"\\nTraining..\")\n",
    "    n_total_steps = len(train_dl)\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        batch_truths, batch_class_labels, batch_reg_labels = batch\n",
    "        batch_truths = torch.tensor(np.array(batch_truths)).t().float().to(device)\n",
    "        batch_class_labels = torch.tensor(np.array(batch_class_labels)).float().to(device)\n",
    "        batch_reg_labels = torch.tensor(np.array(batch_reg_labels)).float().to(device)\n",
    "        class_pred, reg_pred = delay_model(batch_truths)\n",
    "        class_pred = class_pred.squeeze(1)\n",
    "        reg_pred = reg_pred.squeeze(1)\n",
    "        loss = loss_function(class_pred, reg_pred, batch_class_labels, batch_reg_labels)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        train_loss = (train_loss*(i)+loss.item())/(i+1)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{ep+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {train_loss:.4f}')\n",
    "            \n",
    "    print(\"\\nValidation..\")\n",
    "    n_total_steps = len(valid_dl)\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_dl):\n",
    "            batch_truths, batch_class_labels, batch_reg_labels = batch\n",
    "            batch_truths = torch.tensor(np.array(batch_truths)).t().float().to(device)\n",
    "            batch_class_labels = torch.tensor(np.array(batch_class_labels)).float().to(device)\n",
    "            batch_reg_labels = torch.tensor(np.array(batch_reg_labels)).float().to(device)\n",
    "            class_pred, reg_pred = delay_model(batch_truths)\n",
    "            class_pred = class_pred.squeeze(1)\n",
    "            reg_pred = reg_pred.squeeze(1)\n",
    "            loss = loss_function(class_pred, reg_pred, batch_class_labels, batch_reg_labels)\n",
    "            valid_loss = (valid_loss*(i)+loss.item())/(i+1)\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print (f'Epoch [{ep+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    if ep == 0:\n",
    "        optim_valid_loss = valid_loss\n",
    "        torch.save(delay_model.state_dict(), \"model_weights/epoch-{}-val_loss-{:.4f}.pt\".format(ep+1,optim_valid_loss))\n",
    "    else:\n",
    "        if valid_loss < optim_valid_loss:\n",
    "            optim_valid_loss = valid_loss\n",
    "            best_val_epoch = ep+1\n",
    "            torch.save(delay_model.state_dict(), \"model_weights/epoch-{}-val_loss-{:.4f}.pt\".format(ep+1,optim_valid_loss))\n",
    "        \n",
    "    print(\"Training loss for epoch {} is {:.4f}\".format(ep+1,train_loss))\n",
    "    print(\"Validation loss for epoch {} is {:.4f}\".format(ep+1,valid_loss))\n",
    "    train_curve.append(train_loss)\n",
    "    valid_curve.append(valid_loss)\n",
    "    scheduler.step(valid_loss)\n",
    "    end_time = time.time()\n",
    "    epoch_time = (end_time - start_time) / 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priority_cuts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
